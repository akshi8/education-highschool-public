{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predicting On-Time High School Graduation\n",
    "\n",
    "In this notebook, our goal is to produce a useful and descriptive model that identifies students who may not finish high school on time. We can do this using data science methods that produce predictive scores from input student data. On a given set of students, we will conduct an experiment carried out in two trials, each composed of different students. The first trial will be used to train the predictive model, while the second trial will be used to evaluate its efficacy.\n",
    "\n",
    "First, let's some Python packages. `pandas` is a Python package providing data structures designed to make working with labeled data fast and easy. We also import our high school prediction pipeline, `hspipeline`, and relevant functions therein.\n",
    "\n",
    "Second, we can define some basic constants. The input and output directories (`input_dir` and `output_dir`, respectively) determine where input (if applicable) is expected and where output is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import experimental configuration.\n",
    "import config\n",
    "\n",
    "# Import pipeline modules, and database utilities.\n",
    "from hspipeline.pipeline import *\n",
    "from hspipeline.utils.database import connect as db_connect\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.path.abspath('__file__'))\n",
    "input_dir = os.path.join(BASE_PATH, 'input')\n",
    "output_dir = os.path.join(BASE_PATH, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define modeling parameters.\n",
    "\n",
    "`unit_col` specifies the field that defines an individual instance or record.\n",
    "`time_col` specifies the field that defines an individual timestep.\n",
    "`cohort_col` specifies the field that determines how individual instances or records are grouped. This is assumed to be a cohort year and is used for separating instances (i.e., students) into training and testing disjuncts.\n",
    "`label_col` specifies the field that is used as the target variable for prediction.\n",
    "\n",
    "The `fetch_data` function either retrieves data from a saved Python pickle file or retrieves data for the specified district from a PostgreSQL database. It returns `data`, which represents each `unit_col` as a row with features and a `label_col` and, if applicable, returns the categories associated with each feature, `X_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unit_col = 'student_id'\n",
    "time_col = 'grade_level'\n",
    "cohort_col = 'cohort'\n",
    "label_col = 'label'\n",
    "\n",
    "district = 'vps'\n",
    "#pickle_filename = 'data_all_2015_08_20_vps'\n",
    "\n",
    "data, X_categories = fetch_data(district=district, \n",
    "                                from_pickle=False, \n",
    "                                #pickle_filename=os.path.join(input_dir, pickle_filename), \n",
    "                                unit_col=unit_col, \n",
    "                                time_col=time_col\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can perform basic pre-processing to the data prior to modeling. Here, we drop date of birth from the feature list, as well as district-specific fields. We define the features or prediction variables, `X_cols`, ensuring that the label, unit, and time fields are excluded. We also define the target variable, `y_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(['date_of_birth'], 1)\n",
    "\n",
    "if district == 'vps':\n",
    "    pass\n",
    "elif district == 'wcpss':\n",
    "    data = data.drop(['age_first_entered_wcpss'], 1)\n",
    "    data = data.drop(['age_entered_into_us'], 1)\n",
    "\n",
    "X_cols = data.columns[(data.columns != label_col) & (data.columns != unit_col) & (data.columns != time_col)]\n",
    "y_col = label_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run a sweep of predefined models via our `run_models` function. This function will write a variety of output to the specified `output_dir`.\n",
    "\n",
    "`summary.csv` contains one row for each model configuration in the model sweep, providing a summary of the performance results. Within the output directory, a subdirectory, `predictions`, is created that provides additional information for each model. At the termination of a nested folder structure, a file is written for each model configuration in the model sweep. This file contains a row for each `unit_col` (i.e., student) in the testing data for that model with four columns: actual `label_col` (i.e., the label) value, predicted label value, probability for the positive label value, and the `unit_col` identifier.\n",
    "\n",
    "Additionally, a `summary_dummy.csv` file may be created with baseline, \"dummy classifier\" results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_models(data, unit_col, time_col, cohort_col, X_cols, y_col, \n",
    "           X_categories=X_categories, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
